{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UV CDAT Utilities Manual Chapter 3 User Contributed Packages\n",
    "\n",
    "General Utilities : The genutil Package\n",
    "\n",
    "The packages described below are contributions submitted by users. They are provided “as-is” and may not be maintained in the future - unless they are extensively used and the user community considers them critical.\n",
    "\n",
    "© The CDAT software was developed by LLNL. This tutorial was written by Charles Doutriaux. This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.\n",
    "\n",
    "\n",
    "[Download the Jupyter Notebook](uv-cdat_chapter3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Reading-ASCII-text-files-(package-asciidata-)\" data-toc-modified-id=\"Reading-ASCII-text-files-(package-asciidata-)-1\">Reading ASCII text files (package asciidata )</a></span></li><li><span><a href=\"#Reading-binary-data-(package-binaryio)\" data-toc-modified-id=\"Reading-binary-data-(package-binaryio)-2\">Reading binary data (package binaryio)</a></span></li><li><span><a href=\"#Explicit-Orthonormal-Functions-(package-eof)\" data-toc-modified-id=\"Explicit-Orthonormal-Functions-(package-eof)-3\">Explicit Orthonormal Functions (package eof)</a></span></li><li><span><a href=\"#Computing-L-moments-(package-lmoments)\" data-toc-modified-id=\"Computing-L-moments-(package-lmoments)-4\">Computing L-moments (package lmoments)</a></span></li><li><span><a href=\"#Regridding-using-package-regridpack\" data-toc-modified-id=\"Regridding-using-package-regridpack-5\">Regridding using package regridpack</a></span></li><li><span><a href=\"#Using-Spherepack-(package-sphere)\" data-toc-modified-id=\"Using-Spherepack-(package-sphere)-6\">Using Spherepack (package sphere)</a></span></li><li><span><a href=\"#Computing-Trends-(package-trends)\" data-toc-modified-id=\"Computing-Trends-(package-trends)-7\">Computing Trends (package trends)</a></span></li><li><span><a href=\"#Reading-data-from-an-Oort-file-(package-ort)\" data-toc-modified-id=\"Reading-data-from-an-Oort-file-(package-ort)-8\">Reading data from an Oort file (package ort)</a></span></li><li><span><a href=\"#A-grads-like-interface-(package-grads)\" data-toc-modified-id=\"A-grads-like-interface-(package-grads)-9\">A grads like interface (package grads)</a></span></li><li><span><a href=\"#Interface-to-the-ngmath-library.-(package-ngmath)\" data-toc-modified-id=\"Interface-to-the-ngmath-library.-(package-ngmath)-10\">Interface to the ngmath library. (package ngmath)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading ASCII text files (package asciidata )\n",
    "[Back to Top](#top)\n",
    "\n",
    "Package asciidata reads data from ASCII text files.\n",
    "\n",
    "Reads text files written by such programs as spreadsheets, in which data has been written as comma, tab, or space-separated numbers with a header line that names the fields. Using the functions in asciidata, you can convert these columns into Numerical arrays, with control over the type/precision of these arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-8-10de02094391>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-10de02094391>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    time, pressure = asciidata.comma_separated(`myfile.txt')\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    ">>> import asciidata\n",
    ">>> time, pressure = asciidata.comma_separated(`myfile.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For documentation type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w asciidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific Python also contains a subpackage IO that contains other useful facilities of this type. In particular there is a useful package for reading Fortran-like formatted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading binary data (package binaryio)\n",
    "[Back to Top](#top)\n",
    "\n",
    "\n",
    "Read and write Fortran unformatted i/o files.\n",
    "\n",
    "These are the files that you read and write in Fortran with statements like read(7) or write(7). Such files have an unspecified format and are platform and compiler dependent. They are NOT portable. Contrary to popular opinion, they are NOT standard. The standard only specifies their existence and behavior, not the details of their implementation, and since there is no one obvious implementation, Fortran compilers do vary. We suggest writing netcdf files instead, using the facilities in cdms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For documentation type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w binaryio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar package is in Scientific Python.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-276b78a8bc63>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-276b78a8bc63>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    y = binaryio.binread(iunit, n, ...)\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ">>> import binaryio\n",
    ">>> iunit = binaryio.bincreate('filename')\n",
    ">>> binaryio.binwrite(iunit, some_array)\n",
    "#(up to 4 dimensions)\n",
    ">>> binaryio.binclose(iunit)\n",
    ">>> iunit = binaryio.binopen('filename')\n",
    ">>> y = binaryio.binread(iunit, n, ...)\n",
    "# (1-4 dimensions)\n",
    ">>> binaryio.binclose(iunit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reads and writes must be paired exactly. Errors will cause a Fortran STOP that cannot be recovered from. You must know (or have written earlier in the file) the sizes of each array. All data is stored as 32-bit floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Orthonormal Functions (package eof)\n",
    "[Back to Top](#top)\n",
    "\n",
    "Calculates Explicit Orthonormal Functions of either one variable or two variables jointly.\n",
    "\n",
    "Having selected some data, the key call is to create an instance of eof.Eof giving one or two arguments. In this example, a portion of the variable u is analyzed. After the result is returned, it is an object with attributes containing such things as the principal components and the percent of variance explained. Optional arguments are available for controlling the subtraction of the mean from the data, the weighting by latitude, and the number of components to compute.\n",
    "\n",
    "This routine is computationally efficient, solving the problem in either the normal space or the dual space in order to minimize computations. Nonetheless, it is possible that this routine will require substantial time and space if used on a large amount of data. This cost is determined by the smaller of the number of time points and the total number of space points.\n",
    "\n",
    "For documentation type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w eof.Eof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-73f007246a01>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-73f007246a01>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    x.isofill(y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    ">>> import cdms, vcs\n",
    ">>> from eof import Eof\n",
    ">>> f=cdms.open('/home/dubois/clt.nc')\n",
    ">>> u = f('u', latitude=(-20,40), longitude=(60, 120))\n",
    ">>> result = Eof(u)\n",
    ">>> principal_components = result.principal_components\n",
    ">>> print \"Percent explained\", result.percent_explained\n",
    ">>> x=vcs.init()\n",
    ">>> print len(principal_components)\n",
    ">>> for y in principal_components:\n",
    ">>> x.isofill(y)\n",
    ">>> x.clear()\n",
    ">>> u1 = v.subRegion(latitude=(amr[0], \\\n",
    "amr[1], 'cc'), longitude=(amr[2], \\\n",
    "amr[3],'cc'), order='xyt')\n",
    ">>> result2 = Eof(u, number_of_components=4,\\\n",
    "mean_choice=12)\n",
    ">>> print \"Percent explained\", result.percent_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing L-moments (package lmoments)\n",
    "[Back to Top](#top)\n",
    "\n",
    "An interface to an L-moments library by J. R. M. Hosking.\n",
    "\n",
    "This package is an interface to a Fortran library. The calling sequence from Python differs from the Fortran convention. In general, output and temporary arguments are not supplied in making the Python call, and output arguments are returned as values of the function.\n",
    "\n",
    "For documentation type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w lmoments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see list of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w lmoments.pelexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or other function name, for the particular. See also documentation for Pyfort for further details on argument conventions. If built from source, a file flmoments.txt appears which gives the Python calling sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regridding using package regridpack\n",
    "[Back to Top](#top)\n",
    "\n",
    "Interface to regridpack\n",
    "\n",
    "For documentation type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w adamsregrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package contains a Python interface to the subroutine library regridpack.\n",
    "\n",
    "See the documentation. See also documentation for http://pyfortran.sourceforge.net/  for further details on argument conventions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spherepack (package sphere)\n",
    "[Back to Top](#top)\n",
    "\n",
    "Interface to Spherepack. This package contains a Python interface to the subroutine library Spherepack.\n",
    "\n",
    "For documentation type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% pydoc -w sphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see list of functions.\n",
    "\n",
    "See the documentation. See also documentation for http://pyfortran.sourceforge.net/ for further details on argument conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Trends (package trends)\n",
    "[Back to Top](#top)\n",
    "\n",
    "Computes variance estimate taking auto-correlation into account.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-a669a971db08>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a669a971db08>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import reg_arl from trends\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import reg_arl from trends\n",
    "rneff, result, res, cxx, rxx = reg_arl (lag, x, y)\n",
    "integer lag Max lag for autocorrelations.\n",
    "real x(n1) Independent variable\n",
    "real y(n1) Dependent variable\n",
    "real, intent(out):: rneff !Effective sample size\n",
    "real, intent(out):: result(31) !Array of linear regression results\n",
    "real, intent(out):: res(n1) !Residuals from linear regression\n",
    "real, intent(out):: cxx(1 + lag) !Autocovariance function\n",
    "real, intent(out):: rxx(1 + lag) !Autocorrelation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from an Oort file (package ort)\n",
    "[Back to Top](#top)\n",
    "\n",
    "Read data from an Oort file.\n",
    "\n",
    "Module ort contains one Fortran function, read1f:\n",
    "\n",
    "Calling sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-12-c8d6f8f553c3>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-c8d6f8f553c3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    lon, lat, data, nr = ort.read1f(filename, maxsta,\\ nvarbs, nlevels)\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    ">>> import ort\n",
    ">>> lon, lat, data, nr = ort.read1f(filename, maxsta,\\ nvarbs, nlevels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "character*(*) filename ! name of the file to be read\n",
    "! max number of stations (soundings) possible\n",
    "integer maxsta\n",
    "! number of variables and P-levels in each sounding\n",
    "integer nvarbs, nlevels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! longitudes / latitudes of the stations\n",
    "real, intent(out):: lon(maxsta), lat(maxsta)\n",
    "! sounding data\n",
    "real , intent(out):: data(nvarbs, nlevels, maxsta)\n",
    "! actual number of stations with data\n",
    "integer , intent(out):: nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A grads like interface (package grads)\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The grads module supplies an interface to cdms that will be familiar to users of GrADS.\n",
    "\n",
    "See the UV-CDAT website for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface to the ngmath library. (package ngmath)\n",
    "[Back to Top](#top)\n",
    "\n",
    "The ngmath library is a collection of interpolators and approximators for one-dimensional, two-dimensional and three-dimensional data. The packages, which were obtained from NCAR, are:\n",
    "\n",
    "    natgrid - a two-dimensional random data interpolation package based on Dave Watson’s nngridr. NOT built by default in UV-CDAT due to compile problems on some platforms. Works on linux.\n",
    "    dsgrid - a three-dimensional random data interpolator based on a simple inverse distance weighting algorithm.\n",
    "    fitgrid - an interpolation package for one-dimensional and two-dimensional gridded data based on Alan Cline’s Fitpack. Fitpack uses splines under tension to interpolate in one and two dimensions. NOT IN UV-CDAT.\n",
    "\n",
    "    csagrid - an approximation package for one-dimensional, two-dimensional and three-dimensional random data based on David Fulker’s Splpack. csagrid uses cubic splines to calculate its approximation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
